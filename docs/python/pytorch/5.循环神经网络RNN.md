![image-20200415002412123](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200415002412123.png)

```Python
# 定义超参数
batch_size = 100
learning_rate = 1e-3
num_epoches = 20

# 定义 Recurrent Network 模型
class Rnn(nn.Module):
    def __init__(self, in_dim, hidden_dim, n_layer, n_class):
        super(Rnn, self).__init__()
        self.n_layer = n_layer
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(in_dim, hidden_dim, n_layer, batch_first=True)
        self.classifier = nn.Linear(hidden_dim, n_class)

    def forward(self, x):
        # h0 = Variable(torch.zeros(self.n_layer, x.size(1),
        #   self.hidden_dim)).cuda()
        # c0 = Variable(torch.zeros(self.n_layer, x.size(1),
        #   self.hidden_dim)).cuda()
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        out = self.classifier(out)
        return out


model = Rnn(28, 128, 2, 10)  # 图片大小是28x28
use_gpu = torch.cuda.is_available()  # 判断是否有GPU加速
if use_gpu:
    model = model.cuda()
# 定义loss和optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# 开始训练
for epoch in range(num_epoches):
    print('epoch {}'.format(epoch + 1))
    print('*' * 10)
    running_loss = 0.0
    running_acc = 0.0
    for i, data in enumerate(train_loader, 1):
        img, label = data
        b, c, h, w = img.size()
        assert c == 1, 'channel must be 1'
        img = img.squeeze(1)
        # img = img.view(b*h, w)
        # img = torch.transpose(img, 1, 0)
        # img = img.contiguous().view(w, b, -1)
        if use_gpu:
            img = Variable(img).cuda()
            label = Variable(label).cuda()
        else:
            img = Variable(img)
            label = Variable(label)
        # 向前传播
        out = model(img)
        loss = criterion(out, label)
        running_loss += loss.data[0] * label.size(0)
        _, pred = torch.max(out, 1)
        num_correct = (pred == label).sum()
        running_acc += num_correct.data[0]
        # 向后传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if i % 300 == 0:
            print('[{}/{}] Loss: {:.6f}, Acc: {:.6f}'.format(
                epoch + 1, num_epoches, running_loss / (batch_size * i),
                running_acc / (batch_size * i)))
    print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(
        epoch + 1, running_loss / (len(train_dataset)), running_acc / (len(
            train_dataset))))
```

定义好LSTM网络，需要nn.LSTM()，首先介绍一下这个函数里面的参数
```python
"""
Class torch.nn.LSTM(*args, **kwargs)
参数:
- input_size – 输入的数据/特征维度

- hidden_size – 隐状态的特征维度 即输出维数

- num_layers – 层数（和时序展开要区分开） 表示堆叠几层的LSTM，默认是1

- bias – 如果为False，那么LSTM将不会使用bias，默认为True。

- batch_first – 如果为True，那么输入和输出Tensor的形状为(batch, seq, feature)
因为nn.lstm()接受的数据输入是(序列长度，batch，输入维数)，这和我们cnn输入的方式不太一致，所以使用batch_first，我们可以将输入变成(batch，序列长度，输入维数)

- dropout – 如果非零的话，将会在RNN的输出上加个dropout，最后一层除外。

- bidirectional – 如果为True，将会变成一个双向LSTM，默认为False。
表示双向LSTM，也就是序列从左往右算一次，从右往左又算一次，这样就可以两倍的输出
"""

"""
输出:
LSTM输入: input, (h_0, c_0)

- input (seq_len, batch, input_size):
包含输入序列特征的Tensor。也可以是packed variable ，详见 [pack_padded_sequence](#torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False[source])

- h_0 (num_layers * num_directions, batch, hidden_size):
保存着batch中每个元素的初始化隐状态的Tensor

- c_0 (num_layers * num_directions, batch, hidden_size):
保存着batch中每个元素的初始化细胞状态的Tensor
"""

"""
LSTM输出 output, (h_n, c_n)

- output (seq_len, batch, hidden_size * num_directions):
保存RNN最后一层的输出的Tensor。 如果输入是torch.nn.utils.rnn.PackedSequence，那么输出也是torch.nn.utils.rnn.PackedSequence。

- h_n (num_layers * num_directions, batch, hidden_size):
Tensor，保存着RNN最后一个时间步的隐状态。

- c_n (num_layers * num_directions, batch, hidden_size):
Tensor，保存着RNN最后一个时间步的细胞状态。
"""
```
