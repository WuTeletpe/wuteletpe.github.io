## 张量 tensor
```python
import torch

"""
初始化一个 5*4 的 torch的tensor(零矩阵)
"""
torch.Tensor(5, 4)
"""
初始化一个 5*4 的 torch的tensor(随机矩阵)
"""
a = torch.rand(5, 4)
a.size()
```

### numpy to tensor
```python
import numpy as np
"""
初始化一个 1*15 的numpy的array
"""
x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],
                    [9.779], [6.182], [7.59], [2.167], [7.042],
                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)

"""
将 numpy 的 array 转化成 torch 的 tensor
"""
x_train = torch.from_numpy(x_train)
print(x_train)
```

### tensor to numpy
```python
"""
将 torch 的 tensor 转化成 numpy 的 array
"""
a = torch.rand(5, 4)
b = a.numpy()
print(b)
```

### tensor 运算
```python
x = torch.rand(5, 4)
y = torch.rand(5, 4)
c = 3
print(x.add(y))
"""
x+y或者x.add()并不会改变x，
但是x.add_()则会对x进行改变
可以直接进行操作改变原对象，
"""
x.add_(y)
```

### 将 tensor 放到 GPU 上
```python
# 判断一下电脑是否支持GPU
torch.cuda.is_available() # True

a = torch.rand(5, 4)
a = a.cuda()
print(a)
```

## 变量
本质上Variable和Tensor没有什么区别，
不过Variable会放在一个计算图里面，
可以进行前向传播和反向传播以及求导

![image-20200415142407409](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200415142407409.png)

data 表示这个 Variable 是由哪个 tensor 构成的
creator 表示通过什么操作得到的这个 Variable
grad 表示反向传播的梯度
```python
"""
将 torch的tensor 转化成 torch的Variable
requires_grad 表示调用 backward 时是否计算梯度
"""
from torch.autograd import Variable # torch 中 Variable 模块
x = Variable(torch.Tensor([3,]), requires_grad=True)
y = Variable(torch.Tensor([5,]), requires_grad=True)
z = 2 * x + y + 4
print(z)

"""
将 torch的Variable 转化成 torch的tensor
"""
print(x.data)
print(y.data)

# 对 x 和 y 分别求导
z.backward()

# x 的导数和 y 的导数
"""
获取 torch的Variable 的梯度值
"""
print('dz/dx: {}'.format(x.grad.data))
print('dz/dy: {}'.format(y.grad.data))

```
```python
from torch.autograd import Variable

tensor = torch.FloatTensor([[1,2],[3,4]])
variable = Variable(tensor, requires_grad=True) #requires_grad是参不参与误差反向传播, 要不要计算梯度
print(variable)
```
