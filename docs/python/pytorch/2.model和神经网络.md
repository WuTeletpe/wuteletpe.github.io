torch.nn 里面有着所有的神经网络的层的操作，其用来构建网络，只有执行一次网络的运算才执行一次

torch.nn.functional 表示的是直接对其做一次向前运算操作

###  1. 定义 model 类
```python
"""
torch.nn.Module
所有神经网络模块的基类。
"""
from torch import nn
import torch.nn.functional as F

# 基本的网络构建类模板
class net_name(nn.Module):
    def __init__(self):
        super(net_name, self).__init__()
        """
        定义各种层作为成员变量
        """
        # 可以添加各种网络层
        self.conv1 = nn.Conv2d(3, 10, 3)
        # 具体每种层的参数可以去查看文档

    """
    定义向前传播的计算过程
    """
    def forward(self, x):
        out = self.conv1(x)
        return out
```
```python
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # input and output is 1 dimension

    def forward(self, x):
        out = self.linear(x)
        return out

model = LinearRegression()
```

#### 1.1 各种层
```python
"""
nn.Linear
这里的nn.Linear表示的是 y=w*x b
nn.Linear的两个参数都是1，表示的是x是1维，y也是1维。
"""
self.linear = nn.Linear(1, 1)  # input and output is 1 dimension
```

#### 1.2 神经网络例子

![image-20200414115922869](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200414115922869.png)

```python
"""
定义了三层神经网络，输入是28×28，因为图片大小是28×28，中间两个隐藏层大小分别是300和100，最后是个10分类问题，所以输出层为10.
"""
class Neuralnetwork(nn.Module):
    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):
        super(Neuralnetwork, self).__init__()
        self.layer1 = nn.Linear(in_dim, n_hidden_1)
        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)
        self.layer3 = nn.Linear(n_hidden_2, out_dim)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return x

model = Neuralnetwork(28*28, 300, 100, 10)
if torch.cuda.is_available():
    model = model.cuda()
```

### 2. 训练
```python
"""
定义loss和optimizer，就是误差和优化函数
"""
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=1e-4)

"""
开始训练
"""
num_epochs = 1000 # 迭代次数为 1000 次
for epoch in range(num_epochs):
    inputs = Variable(x_train)
    target = Variable(y_train)

    # forward
    out = model(inputs) # 前向传播
    loss = criterion(out, target) # 计算loss

    # backward
    optimizer.zero_grad() # 每次反向传播前要将参数的梯度归零
    loss.backward() # 反向传播计算梯度
    optimizer.step() # 更新参数

    if (epoch + 1) % 20 == 0:
        print('Epoch[{}/{}], loss: {:.6f}'.format(epoch + 1, num_epochs, loss.data[0]))
```
#### 2.1 各种损失函数
```python
"""
最小二乘loss，之后我们做分类问题更多的使用的是cross entropy loss，交叉熵。
"""
criterion = nn.MSELoss()

"""
交叉熵loss。
"""
criterion = nn.CrossEntropyLoss()
```

#### 2.2 各种优化函数
```python
"""
优化函数使用的是随机梯度下降
"""
optimizer = optim.SGD(model.parameters(), lr=1e-4)

"""
优化函数使用Adam(Adaptive Moment Estimation)
本质上是带有动量项的RMSprop
"""
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

```

### 2.3 nn.Module 的函数
```python
"""
parameters()
返回类型: class torch.nn.Parameter()
返回模块参数的迭代器。 这通常被传递给优化器。
"""
for param in model.parameters():
    print(type(param.data), param.size())

<class 'torch.FloatTensor'> (20L,)
<class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)

"""
class torch.nn.Parameter()
一种Variable，被视为一个模块参数。
Parameters 是 Variable 的子类。
当与Module一起使用时，它们具有非常特殊的属性，
当它们被分配为模块属性时，它们被自动添加到其参数列表中，并将出现在例如parameters()迭代器中。
"""
```
### 3. 测试
```python
"""
用 model.eval()，让model变成测试模式，
这主要是对dropout和batch normalization的操作在训练和测试的时候是不一样的
"""
model.eval()
predict = model(Variable(x_train))
predict = predict.data.numpy()
```
